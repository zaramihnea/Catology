{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:59:07.899705Z",
     "start_time": "2024-12-26T16:59:05.415377Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def create_dataframe(path):\n",
    "    df = pd.read_excel(path)\n",
    "    return df\n",
    "\n",
    "def delete_duplicates(df):\n",
    "    # Stergerea duplicatelor\n",
    "    df.drop_duplicates(inplace = True) # inplace = True indica faptul ca fuctia va sterge duplicate din dataframe-ul dat si nu va return unul nou\n",
    "    return df\n",
    "\n",
    "def remove_unknown_gender_instances(df):\n",
    "    # Eliminăm rândurile unde 'Gender' are valoarea 'Unknown'\n",
    "    return df[df['Gender'] != 'Unknown']\n",
    "\n",
    "def remove_unknown_race_instances(df):\n",
    "    # Eliminăm rândurile unde 'Race' are valoarea 'Unknown'\n",
    "    return df[df['Race'] != 'Unknown']\n",
    "\n",
    "def replace_unknown_with_median(x):\n",
    "    # Înlocuiește \"Unknown\" cu mediană, după ce valorile sunt convertite în numeric\n",
    "    median_value = pd.to_numeric(x[x != \"Unknown\"]).median()\n",
    "    return x.replace(\"Unknown\", median_value)\n",
    "\n",
    "# Funcția pentru aplicarea pe grupuri\n",
    "def edit_unknown_values_for_natural_area(df):\n",
    "    df[\"The abundance of natural areas\"] = df.groupby(\"Race\")[\"The abundance of natural areas\"].transform(\n",
    "        lambda x: replace_unknown_with_median(x)\n",
    "    )\n",
    "\n",
    "    # Convertim în int\n",
    "    df[\"The abundance of natural areas\"] = df[\"The abundance of natural areas\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def encode_age(df):\n",
    "    # Create a copy of the DataFrame\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    # Age mapping\n",
    "    age_mapping = {\n",
    "        'Less than 1 year': 0.5,\n",
    "        '1-2 years': 1.5,\n",
    "        '2-10 years': 6,\n",
    "        'More than 10 years': 12\n",
    "    }\n",
    "    \n",
    "    # Apply age mapping\n",
    "    df_transformed['Age'] = df_transformed['Age'].map(age_mapping)\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "def encode_categorical_columns(df, categorical_columns):\n",
    "    # Create one-hot encoded columns\n",
    "    for column in categorical_columns:\n",
    "        one_hot = pd.get_dummies(df[column], prefix=column)\n",
    "        \n",
    "        # Add one-hot encoded columns to the transformed DataFrame\n",
    "        df = pd.concat([df, one_hot], axis=1)\n",
    "        \n",
    "        # Drop the original categorical column\n",
    "        df = df.drop(column, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocessing():\n",
    "    # Crearea dataframe-ului\n",
    "    df = create_dataframe('./Dataset/Dataset.xlsx')\n",
    "\n",
    "    #stergerea duplicatelor\n",
    "    df = delete_duplicates(df)\n",
    "\n",
    "    # Stergerea instantelor unde Gender = Unknown (6 instante)\n",
    "    df = remove_unknown_gender_instances(df)\n",
    "\n",
    "    # Stergerea instantelor unde Race = Unknown (79 instante)\n",
    "    df = remove_unknown_race_instances(df)\n",
    "\n",
    "    # modificam instantele ce au valori 'Unknown' pentru coloana 'The abundance of natural areas' in mediana pentru fiecare rasa (240 de instante)\n",
    "    df = edit_unknown_values_for_natural_area(df)\n",
    "\n",
    "    # encodam coloana 'Age'\n",
    "    df = encode_age(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocessing()\n",
    "df.info()\n",
    "# Print current path\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3055 entries, 0 to 3142\n",
      "Data columns (total 26 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   Gender                              3055 non-null   object \n",
      " 1   Age                                 3055 non-null   float64\n",
      " 2   Race                                3055 non-null   object \n",
      " 3   Number of cats in the household     3055 non-null   int64  \n",
      " 4   Type of housing                     3055 non-null   object \n",
      " 5   Zone                                3055 non-null   object \n",
      " 6   Time spent outside each day         3055 non-null   int64  \n",
      " 7   Time spent with the owner each day  3055 non-null   int64  \n",
      " 8   Shy                                 3055 non-null   int64  \n",
      " 9   Calm                                3055 non-null   int64  \n",
      " 10  Skittish                            3055 non-null   int64  \n",
      " 11  Intelligent                         3055 non-null   int64  \n",
      " 12  Vigilant                            3055 non-null   int64  \n",
      " 13  Tenacious                           3055 non-null   int64  \n",
      " 14  Affectionate                        3055 non-null   int64  \n",
      " 15  Friendly                            3055 non-null   int64  \n",
      " 16  Loner                               3055 non-null   int64  \n",
      " 17  Ferocious                           3055 non-null   int64  \n",
      " 18  Territorial                         3055 non-null   int64  \n",
      " 19  Aggressive                          3055 non-null   int64  \n",
      " 20  Impulsive                           3055 non-null   int64  \n",
      " 21  Predictable                         3055 non-null   int64  \n",
      " 22  Inattentive                         3055 non-null   int64  \n",
      " 23  The abundance of natural areas      3055 non-null   int32  \n",
      " 24  Frequency of Bird Captures          3055 non-null   int64  \n",
      " 25  Frequency of Small Mammal Captures  3055 non-null   int64  \n",
      "dtypes: float64(1), int32(1), int64(20), object(4)\n",
      "memory usage: 632.5+ KB\n"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Antrenarea pe setul de date cu modificări minime."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:59:07.979650Z",
     "start_time": "2024-12-26T16:59:07.932391Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, input_data, training=True):\n",
    "        if training:\n",
    "            self.mask = np.random.binomial(1, 1 - self.dropout_rate, input_data.shape) / (1 - self.dropout_rate)\n",
    "            return input_data * self.mask\n",
    "        return input_data\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.mask\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation='relu', l2_lambda=0.005, dropout_rate=0.2):\n",
    "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2.0/input_size)\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.activation = activation\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.z = None\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return {\n",
    "            'weights': self.weights.copy(),\n",
    "            'bias': self.bias.copy()\n",
    "        }\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        self.weights = weights['weights'].copy()\n",
    "        self.bias = weights['bias'].copy()\n",
    "        \n",
    "    def forward(self, input_data, training=True):\n",
    "        self.input = input_data\n",
    "        self.z = np.dot(input_data, self.weights) + self.bias\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            self.output = np.maximum(0, self.z)\n",
    "        elif self.activation == 'softmax':\n",
    "            exp_values = np.exp(self.z - np.max(self.z, axis=1, keepdims=True))\n",
    "            self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        if self.activation != 'softmax':\n",
    "            self.output = self.dropout.forward(self.output, training)\n",
    "            \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        if self.activation != 'softmax':\n",
    "            grad_output = self.dropout.backward(grad_output)\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            grad_z = grad_output * (self.z > 0)\n",
    "        elif self.activation == 'softmax':\n",
    "            grad_z = grad_output\n",
    "            \n",
    "        grad_weights = np.dot(self.input.T, grad_z) + self.l2_lambda * self.weights\n",
    "        grad_bias = np.sum(grad_z, axis=0, keepdims=True)\n",
    "        grad_input = np.dot(grad_z, self.weights.T)\n",
    "        \n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.bias -= learning_rate * grad_bias\n",
    "        \n",
    "        return grad_input\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_dim, output_dim, l2_lambda=0.005):\n",
    "        self.layers = [\n",
    "            Layer(input_dim, 128, 'relu', l2_lambda, dropout_rate=0.1),\n",
    "            Layer(128, 64, 'relu', l2_lambda, dropout_rate=0.1),\n",
    "            Layer(64, output_dim, 'softmax', l2_lambda, dropout_rate=0.0)\n",
    "        ]\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return [layer.get_weights() for layer in self.layers]\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        for layer, w in zip(self.layers, weights):\n",
    "            layer.set_weights(w)\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        current_output = X\n",
    "        for layer in self.layers:\n",
    "            current_output = layer.forward(current_output, training)\n",
    "        return current_output\n",
    "    \n",
    "    def backward(self, X, y, learning_rate):\n",
    "        grad_output = self.layers[-1].output - y\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output, learning_rate)\n",
    "    \n",
    "    def train_step(self, X_batch, y_batch, learning_rate):\n",
    "        predictions = self.forward(X_batch, training=True)\n",
    "        self.backward(X_batch, y_batch, learning_rate)\n",
    "        loss = self.compute_loss(predictions, y_batch)\n",
    "        return loss, predictions\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X, training=False)\n",
    "    \n",
    "    def compute_loss(self, predictions, y_true):\n",
    "        epsilon = 1e-15\n",
    "        predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "        return -np.mean(np.sum(y_true * np.log(predictions), axis=1))\n",
    "    \n",
    "    def compute_accuracy(self, predictions, y_true):\n",
    "        return np.mean(np.argmax(predictions, axis=1) == np.argmax(y_true, axis=1))\n",
    "\n",
    "def plot_training_history(histories, n_splits):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for fold in range(n_splits):\n",
    "        plt.plot(histories[fold]['train_loss'], label=f'Fold {fold+1}')\n",
    "    plt.title('Training Loss per Fold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot validation accuracy\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for fold in range(n_splits):\n",
    "        plt.plot(histories[fold]['val_acc'], label=f'Fold {fold+1}')\n",
    "    plt.title('Validation Accuracy per Fold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def train_and_evaluate_model(X, y, n_splits=6, epochs=1000, batch_size=32, learning_rate=0.0001, patience=100):\n",
    "    y_encoded = y.values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)  # Fit scaler on all breed_predictor_model\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_results = []\n",
    "    histories = []\n",
    "    \n",
    "    n_classes = y_encoded.shape[1]\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, np.argmax(y_encoded, axis=1)), 1):\n",
    "        print(f\"\\nFold {fold}/{n_splits}\")\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx].values, X.iloc[val_idx].values\n",
    "        y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
    "        \n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        model = NeuralNetwork(X_train.shape[1], n_classes, 0.001)\n",
    "        \n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': []\n",
    "        }\n",
    "        \n",
    "        best_val_accuracy = 0\n",
    "        best_weights = None\n",
    "        best_epoch = 0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(len(X_train_scaled))\n",
    "            X_train_shuffled = X_train_scaled[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "            \n",
    "            epoch_train_loss = 0\n",
    "            epoch_train_acc = 0\n",
    "            n_batches = 0\n",
    "            \n",
    "            for i in range(0, len(X_train_scaled), batch_size):\n",
    "                X_batch = X_train_shuffled[i:i+batch_size]\n",
    "                y_batch = y_train_shuffled[i:i+batch_size]\n",
    "                \n",
    "                loss, predictions = model.train_step(X_batch, y_batch, learning_rate)\n",
    "                accuracy = model.compute_accuracy(predictions, y_batch)\n",
    "                \n",
    "                epoch_train_loss += loss\n",
    "                epoch_train_acc += accuracy\n",
    "                n_batches += 1\n",
    "            \n",
    "            avg_train_loss = epoch_train_loss / n_batches\n",
    "            avg_train_acc = epoch_train_acc / n_batches\n",
    "            \n",
    "            val_predictions = model.predict(X_val_scaled)\n",
    "            val_loss = model.compute_loss(val_predictions, y_val)\n",
    "            val_accuracy = model.compute_accuracy(val_predictions, y_val)\n",
    "            \n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['train_acc'].append(avg_train_acc)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_accuracy)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_weights = model.get_weights()\n",
    "                best_epoch = epoch\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping triggered at epoch {epoch + 1}\")\n",
    "                print(f\"No improvement in validation accuracy for {patience} epochs\")\n",
    "                break\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "                print(f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}\")\n",
    "                print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Restore best weights for this fold\n",
    "        model.set_weights(best_weights)\n",
    "        histories.append(history)\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold,\n",
    "            'best_val_accuracy': best_val_accuracy,\n",
    "            'best_epoch': best_epoch,\n",
    "            'model': model\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nFold {fold} Best Results:\")\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "        print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "    \n",
    "    # Find the best overall model\n",
    "    best_fold = max(fold_results, key=lambda x: x['best_val_accuracy'])\n",
    "    \n",
    "    print(\"\\nOverall Cross-validation results:\")\n",
    "    print(f\"Mean validation accuracy: {np.mean([r['best_val_accuracy'] for r in fold_results]):.4f}\")\n",
    "    print(f\"Standard deviation: {np.std([r['best_val_accuracy'] for r in fold_results]):.4f}\")\n",
    "    print(f\"\\nBest model from fold {best_fold['fold']} at epoch {best_fold['best_epoch']}\")\n",
    "    print(f\"Best validation accuracy: {best_fold['best_val_accuracy']:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(histories, n_splits)\n",
    "    \n",
    "    return best_fold['model'], fold_results, histories, scaler\n",
    "\n",
    "def save_model(model, scaler, filepath='cat_breed_model.pkl'):\n",
    "    \"\"\"Save the model and scaler to a file\"\"\"\n",
    "    model_data = {\n",
    "        'weights': model.get_weights(),\n",
    "        'scaler': scaler\n",
    "    }\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "\n",
    "def prepare_data(data):\n",
    "    race_columns = [col for col in data.columns if col.startswith('Race_')]\n",
    "    X = data.drop(columns=race_columns)\n",
    "    y = data[race_columns]\n",
    "    return X, y\n",
    "\n",
    "def train(data):\n",
    "    X, y = prepare_data(data)\n",
    "    best_model, fold_results, histories, scaler = train_and_evaluate_model(X, y)\n",
    "    save_model(best_model, scaler)\n",
    "    return best_model, fold_results, histories, scaler\n"
   ],
   "outputs": [],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:59:08.038615Z",
     "start_time": "2024-12-26T16:59:08.024428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_model(best_model, test_df):\n",
    "    \"\"\"\n",
    "    Test the trained model on a new test dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    best_model: trained Keras model\n",
    "    test_df: pandas DataFrame containing test breed_predictor_model\n",
    "    \n",
    "    Returns:\n",
    "    dict containing test accuracy, predictions, and evaluation metrics\n",
    "    \"\"\"\n",
    "    # Prepare test breed_predictor_model\n",
    "    race_columns = [col for col in test_df.columns if col.startswith('Race_')]\n",
    "    X_test = test_df.drop(columns=race_columns)\n",
    "    y_test = test_df[race_columns]\n",
    "    \n",
    "    # Scale features using the same approach as training\n",
    "    scaler = StandardScaler()\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = best_model.predict(X_test_scaled)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    y_test_classes = np.argmax(y_test.values, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    test_accuracy = np.mean(y_pred == y_test_classes)\n",
    "    \n",
    "    # Generate classification report\n",
    "    class_names = [col.replace('Race_', '') for col in race_columns]\n",
    "    report = classification_report(y_test_classes, y_pred, target_names=class_names)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test_classes, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    per_class_metrics = {}\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        true_class = (y_test_classes == i)\n",
    "        pred_class = (y_pred == i)\n",
    "        \n",
    "        true_positives = np.sum(true_class & pred_class)\n",
    "        false_positives = np.sum(~true_class & pred_class)\n",
    "        false_negatives = np.sum(true_class & ~pred_class)\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        per_class_metrics[class_name] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': cm,\n",
    "        'per_class_metrics': per_class_metrics,\n",
    "        'predictions': y_pred,\n",
    "        'prediction_probabilities': y_pred_proba\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "def evaluate_test_results(test_df, best_model):\n",
    "    \"\"\"\n",
    "    Evaluate and print test results in a readable format.\n",
    "    \"\"\"\n",
    "    results = test_model(best_model, test_df)\n",
    "    \n",
    "    print(\"\\n=== Model Evaluation on Test Data ===\")\n",
    "    print(f\"\\nOverall Test Accuracy: {results['test_accuracy']:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(results['classification_report'])\n",
    "    \n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    for class_name, metrics in results['per_class_metrics'].items():\n",
    "        print(f\"\\n{class_name}:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"  {metric_name}: {value:.4f}\")\n",
    "    \n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:59:08.090393Z",
     "start_time": "2024-12-26T16:59:08.076729Z"
    }
   },
   "cell_type": "code",
   "source": "print(df.info())",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3055 entries, 0 to 3142\n",
      "Data columns (total 26 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   Gender                              3055 non-null   object \n",
      " 1   Age                                 3055 non-null   float64\n",
      " 2   Race                                3055 non-null   object \n",
      " 3   Number of cats in the household     3055 non-null   int64  \n",
      " 4   Type of housing                     3055 non-null   object \n",
      " 5   Zone                                3055 non-null   object \n",
      " 6   Time spent outside each day         3055 non-null   int64  \n",
      " 7   Time spent with the owner each day  3055 non-null   int64  \n",
      " 8   Shy                                 3055 non-null   int64  \n",
      " 9   Calm                                3055 non-null   int64  \n",
      " 10  Skittish                            3055 non-null   int64  \n",
      " 11  Intelligent                         3055 non-null   int64  \n",
      " 12  Vigilant                            3055 non-null   int64  \n",
      " 13  Tenacious                           3055 non-null   int64  \n",
      " 14  Affectionate                        3055 non-null   int64  \n",
      " 15  Friendly                            3055 non-null   int64  \n",
      " 16  Loner                               3055 non-null   int64  \n",
      " 17  Ferocious                           3055 non-null   int64  \n",
      " 18  Territorial                         3055 non-null   int64  \n",
      " 19  Aggressive                          3055 non-null   int64  \n",
      " 20  Impulsive                           3055 non-null   int64  \n",
      " 21  Predictable                         3055 non-null   int64  \n",
      " 22  Inattentive                         3055 non-null   int64  \n",
      " 23  The abundance of natural areas      3055 non-null   int32  \n",
      " 24  Frequency of Bird Captures          3055 non-null   int64  \n",
      " 25  Frequency of Small Mammal Captures  3055 non-null   int64  \n",
      "dtypes: float64(1), int32(1), int64(20), object(4)\n",
      "memory usage: 632.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 153
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:59:08.267573Z",
     "start_time": "2024-12-26T16:59:08.171502Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definim lungimi posibile ale părului pentru fiecare categorie\n",
    "hair_length_mapping = {\n",
    "    'Sphynx': ['No hair'],\n",
    "    'Siamese': ['Short hair', 'Medium hair'],\n",
    "    'British Shorthair': ['Short hair'],\n",
    "    'Bengal': ['Short hair'],\n",
    "    'Chartreux': ['Short hair'],\n",
    "    'Savannah': ['Short hair'],\n",
    "    'European': ['Short hair', 'Medium hair'],\n",
    "    'Birman': ['Medium hair', 'Long hair'],\n",
    "    'Turkish angora': ['Medium hair', 'Long hair'],\n",
    "    'Persian': ['Long hair'],\n",
    "    'Maine coon': ['Long hair'],\n",
    "    'Ragdoll': ['Medium hair', 'Long hair'],\n",
    "    'No breed': ['Short hair', 'Medium hair'],\n",
    "    'Other': ['Short hair', 'Medium hair']\n",
    "}\n",
    "\n",
    "# Funcție pentru a selecta aleator lungimea părului bazat pe rasa specifică\n",
    "def assign_coat_length(race):\n",
    "    if race in hair_length_mapping:\n",
    "        return np.random.choice(hair_length_mapping[race])\n",
    "    return \"Unknown\"  # Default, în cazul în care rasa nu este cunoscută\n",
    "\n",
    "# Aplicăm funcția pentru fiecare instanță\n",
    "df['Coat Length'] = df['Race'].apply(assign_coat_length)"
   ],
   "outputs": [],
   "execution_count": 154
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:59:08.407477Z",
     "start_time": "2024-12-26T16:59:08.304774Z"
    }
   },
   "source": [
    "# Definirea caracteristicilor legate de coat pattern pentru fiecare rasă\n",
    "coat_patterns = {\n",
    "    \"European\": [\"Tabby\", \"Solid\", \"Bicolor\", \"Tortoiseshell\"],\n",
    "    \"Bengal\": [\"Tabby\"],\n",
    "    \"Ragdoll\": [\"Colorpoint\"],\n",
    "    \"Maine coon\": [\"Tabby\", \"Bicolor\"],\n",
    "    \"Birman\": [\"Colorpoint\"],\n",
    "    \"Persian\": [\"Colorpoint\"],\n",
    "    \"British Shorthair\": [\"Solid\", \"Tabby\", \"Tortoiseshell\"],\n",
    "    \"Sphynx\": [\"Solid\", \"Tabby\"],\n",
    "    \"Siamese\": [\"Colorpoint\"],\n",
    "    \"Chartreux\": [\"Solid\"],\n",
    "    \"Turkish angora\": [\"Solid\", \"Tabby\"],\n",
    "    \"Savannah\": [\"Tabby\"],\n",
    "    \"No breed\": [\"Tabby\", \"Solid\", \"Bicolor\", \"Tricolor\", \"Tortoiseshell\"],\n",
    "    \"Other\": [\"Tabby\", \"Solid\", \"Bicolor\"]\n",
    "}\n",
    "\n",
    "# Adăugarea caracteristicilor în mod uniform\n",
    "df[\"Coat Pattern\"] = df[\"Race\"].apply(\n",
    "    lambda breed: np.random.choice(coat_patterns[breed])  # Atribuim aleatoriu un pattern din lista corespunzătoare\n",
    ")\n",
    "\n",
    "print(df)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Gender   Age        Race  Number of cats in the household  \\\n",
      "0         F   0.5      Birman                                3   \n",
      "1         F   0.5      Birman                                1   \n",
      "2         F   6.0    European                                4   \n",
      "3         F   0.5    European                                1   \n",
      "4         F   1.5      Birman                                2   \n",
      "...     ...   ...         ...                              ...   \n",
      "3138      F   6.0     Persian                                1   \n",
      "3139      F   0.5  Maine coon                                3   \n",
      "3140      M  12.0       Other                                1   \n",
      "3141      M   0.5      Bengal                                1   \n",
      "3142      F   0.5      Bengal                                5   \n",
      "\n",
      "                        Type of housing       Zone  \\\n",
      "0             Apartment without balcony      Urban   \n",
      "1     Apartment with balcony or terrace      Urban   \n",
      "2                House in a subdivision      Urban   \n",
      "3                House in a subdivision      Rural   \n",
      "4                 Individual house zone      Rural   \n",
      "...                                 ...        ...   \n",
      "3138              Individual house zone      Rural   \n",
      "3139              Individual house zone      Rural   \n",
      "3140              Individual house zone  Periurban   \n",
      "3141             House in a subdivision      Rural   \n",
      "3142             House in a subdivision  Periurban   \n",
      "\n",
      "      Time spent outside each day  Time spent with the owner each day  Shy  \\\n",
      "0                               0                                   0    1   \n",
      "1                               0                                   2    1   \n",
      "2                               0                                   2    4   \n",
      "3                               2                                   2    3   \n",
      "4                               1                                   2    1   \n",
      "...                           ...                                 ...  ...   \n",
      "3138                            0                                   3    4   \n",
      "3139                            0                                   3    2   \n",
      "3140                            2                                   3    1   \n",
      "3141                            0                                   2    2   \n",
      "3142                            0                                   2    2   \n",
      "\n",
      "      Calm  ...  Territorial  Aggressive  Impulsive  Predictable  Inattentive  \\\n",
      "0        1  ...            1           1          1            1            1   \n",
      "1        1  ...            2           3          4            4            3   \n",
      "2        4  ...            1           1          2            4            2   \n",
      "3        2  ...            3           3          3            4            4   \n",
      "4        4  ...            4           1          4            3            3   \n",
      "...    ...  ...          ...         ...        ...          ...          ...   \n",
      "3138     2  ...            1           1          1            3            4   \n",
      "3139     2  ...            2           1          2            2            2   \n",
      "3140     4  ...            4           4          3            4            2   \n",
      "3141     4  ...            2           3          1            3            5   \n",
      "3142     2  ...            5           4          5            2            3   \n",
      "\n",
      "      The abundance of natural areas  Frequency of Bird Captures  \\\n",
      "0                                  2                           4   \n",
      "1                                  2                           0   \n",
      "2                                  3                           0   \n",
      "3                                  3                           0   \n",
      "4                                  3                           0   \n",
      "...                              ...                         ...   \n",
      "3138                               3                           0   \n",
      "3139                               3                           0   \n",
      "3140                               3                           0   \n",
      "3141                               3                           0   \n",
      "3142                               3                           0   \n",
      "\n",
      "      Frequency of Small Mammal Captures  Coat Length  Coat Pattern  \n",
      "0                                      4    Long hair    Colorpoint  \n",
      "1                                      0  Medium hair    Colorpoint  \n",
      "2                                      0  Medium hair       Bicolor  \n",
      "3                                      0  Medium hair       Bicolor  \n",
      "4                                      0    Long hair    Colorpoint  \n",
      "...                                  ...          ...           ...  \n",
      "3138                                   0    Long hair    Colorpoint  \n",
      "3139                                   0    Long hair       Bicolor  \n",
      "3140                                   1  Medium hair         Tabby  \n",
      "3141                                   0   Short hair         Tabby  \n",
      "3142                                   0   Short hair         Tabby  \n",
      "\n",
      "[3055 rows x 28 columns]\n"
     ]
    }
   ],
   "execution_count": 155
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:59:08.485474Z",
     "start_time": "2024-12-26T16:59:08.445382Z"
    }
   },
   "source": [
    "# One-hot encode categorical variables\n",
    "categorical_columns = ['Type of housing', 'Zone', 'Race', 'Gender', 'Coat Length', 'Coat Pattern']\n",
    "df = encode_categorical_columns(df, categorical_columns)\n",
    "\n",
    "# Add FlatFace attribute (True for Persian cats)\n",
    "df['FlatFace'] = df['Race_Persian']\n",
    "\n",
    "print(df.info())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3055 entries, 0 to 3142\n",
      "Data columns (total 56 columns):\n",
      " #   Column                                             Non-Null Count  Dtype  \n",
      "---  ------                                             --------------  -----  \n",
      " 0   Age                                                3055 non-null   float64\n",
      " 1   Number of cats in the household                    3055 non-null   int64  \n",
      " 2   Time spent outside each day                        3055 non-null   int64  \n",
      " 3   Time spent with the owner each day                 3055 non-null   int64  \n",
      " 4   Shy                                                3055 non-null   int64  \n",
      " 5   Calm                                               3055 non-null   int64  \n",
      " 6   Skittish                                           3055 non-null   int64  \n",
      " 7   Intelligent                                        3055 non-null   int64  \n",
      " 8   Vigilant                                           3055 non-null   int64  \n",
      " 9   Tenacious                                          3055 non-null   int64  \n",
      " 10  Affectionate                                       3055 non-null   int64  \n",
      " 11  Friendly                                           3055 non-null   int64  \n",
      " 12  Loner                                              3055 non-null   int64  \n",
      " 13  Ferocious                                          3055 non-null   int64  \n",
      " 14  Territorial                                        3055 non-null   int64  \n",
      " 15  Aggressive                                         3055 non-null   int64  \n",
      " 16  Impulsive                                          3055 non-null   int64  \n",
      " 17  Predictable                                        3055 non-null   int64  \n",
      " 18  Inattentive                                        3055 non-null   int64  \n",
      " 19  The abundance of natural areas                     3055 non-null   int32  \n",
      " 20  Frequency of Bird Captures                         3055 non-null   int64  \n",
      " 21  Frequency of Small Mammal Captures                 3055 non-null   int64  \n",
      " 22  Type of housing_Apartment with balcony or terrace  3055 non-null   bool   \n",
      " 23  Type of housing_Apartment without balcony          3055 non-null   bool   \n",
      " 24  Type of housing_House in a subdivision             3055 non-null   bool   \n",
      " 25  Type of housing_Individual house zone              3055 non-null   bool   \n",
      " 26  Zone_Periurban                                     3055 non-null   bool   \n",
      " 27  Zone_Rural                                         3055 non-null   bool   \n",
      " 28  Zone_Urban                                         3055 non-null   bool   \n",
      " 29  Race_Bengal                                        3055 non-null   bool   \n",
      " 30  Race_Birman                                        3055 non-null   bool   \n",
      " 31  Race_British Shorthair                             3055 non-null   bool   \n",
      " 32  Race_Chartreux                                     3055 non-null   bool   \n",
      " 33  Race_European                                      3055 non-null   bool   \n",
      " 34  Race_Maine coon                                    3055 non-null   bool   \n",
      " 35  Race_No breed                                      3055 non-null   bool   \n",
      " 36  Race_Other                                         3055 non-null   bool   \n",
      " 37  Race_Persian                                       3055 non-null   bool   \n",
      " 38  Race_Ragdoll                                       3055 non-null   bool   \n",
      " 39  Race_Savannah                                      3055 non-null   bool   \n",
      " 40  Race_Siamese                                       3055 non-null   bool   \n",
      " 41  Race_Sphynx                                        3055 non-null   bool   \n",
      " 42  Race_Turkish angora                                3055 non-null   bool   \n",
      " 43  Gender_F                                           3055 non-null   bool   \n",
      " 44  Gender_M                                           3055 non-null   bool   \n",
      " 45  Coat Length_Long hair                              3055 non-null   bool   \n",
      " 46  Coat Length_Medium hair                            3055 non-null   bool   \n",
      " 47  Coat Length_No hair                                3055 non-null   bool   \n",
      " 48  Coat Length_Short hair                             3055 non-null   bool   \n",
      " 49  Coat Pattern_Bicolor                               3055 non-null   bool   \n",
      " 50  Coat Pattern_Colorpoint                            3055 non-null   bool   \n",
      " 51  Coat Pattern_Solid                                 3055 non-null   bool   \n",
      " 52  Coat Pattern_Tabby                                 3055 non-null   bool   \n",
      " 53  Coat Pattern_Tortoiseshell                         3055 non-null   bool   \n",
      " 54  Coat Pattern_Tricolor                              3055 non-null   bool   \n",
      " 55  FlatFace                                           3055 non-null   bool   \n",
      "dtypes: bool(34), float64(1), int32(1), int64(20)\n",
      "memory usage: 638.4 KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 156
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:59:09.180448Z",
     "start_time": "2024-12-26T16:59:08.531820Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def remove_race_outliers(df, percentage=5):\n",
    "    \"\"\"\n",
    "    Remove outliers for each cat race based on numerical columns.\n",
    "    Shows distribution of races before and after outlier removal.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe\n",
    "    percentage (float): Percentage of outliers to remove (default 5%)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with outliers removed\n",
    "    \"\"\"\n",
    "    # Get list of race columns and numerical columns\n",
    "    race_columns = [col for col in df.columns if col.startswith('Race_')]\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    numerical_cols = [col for col in numerical_cols if not col.startswith('Race_')]\n",
    "    \n",
    "    # Calculate initial race distribution\n",
    "    print(\"\\nInitial Race Distribution:\")\n",
    "    initial_dist = {}\n",
    "    for race_col in race_columns:\n",
    "        race_count = df[race_col].sum()\n",
    "        race_pct = (race_count / len(df)) * 100\n",
    "        initial_dist[race_col.replace('Race_', '')] = (race_count, race_pct)\n",
    "        print(f\"{race_col.replace('Race_', ''):15} {race_count:5d} cats ({race_pct:5.1f}%)\")\n",
    "    \n",
    "    # Create mask for final filtering\n",
    "    final_mask = pd.Series(True, index=df.index)\n",
    "    \n",
    "    for race_col in race_columns:\n",
    "        # Get subset of breed_predictor_model for this race\n",
    "        race_mask = df[race_col]\n",
    "        race_data = df[race_mask]\n",
    "        \n",
    "        if len(race_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Calculate outlier scores for each numerical column\n",
    "        outlier_scores = pd.DataFrame()\n",
    "        \n",
    "        for col in numerical_cols:\n",
    "            if race_data[col].nunique() > 1:  # Only process if there's variation\n",
    "                Q1 = race_data[col].quantile(0.25)\n",
    "                Q3 = race_data[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                \n",
    "                # Calculate z-scores for more robust outlier detection\n",
    "                z_scores = np.abs((race_data[col] - race_data[col].mean()) / race_data[col].std())\n",
    "                outlier_scores[col] = z_scores\n",
    "        \n",
    "        if not outlier_scores.empty:\n",
    "            # Calculate mean outlier score across all columns\n",
    "            mean_outlier_score = outlier_scores.mean(axis=1)\n",
    "            \n",
    "            # Calculate threshold for given percentage\n",
    "            threshold = mean_outlier_score.quantile(1 - percentage/100)\n",
    "            \n",
    "            # Update final mask\n",
    "            race_indices = race_data.index[mean_outlier_score <= threshold]\n",
    "            final_mask.loc[race_mask] = race_data.index.isin(race_indices)\n",
    "    \n",
    "    # Apply final mask to remove outliers\n",
    "    cleaned_df = df[final_mask].copy()\n",
    "    \n",
    "    # Print summary of removed records\n",
    "    removed_count = len(df) - len(cleaned_df)\n",
    "    print(f\"\\nRemoved {removed_count} records ({(removed_count/len(df)*100):.1f}% of total)\")\n",
    "    \n",
    "    # Calculate final race distribution\n",
    "    print(\"\\nFinal Race Distribution:\")\n",
    "    print(\"Race            Initial Count (%)    Final Count (%)    Change\")\n",
    "    print(\"-\" * 65)\n",
    "    for race_col in race_columns:\n",
    "        race_name = race_col.replace('Race_', '')\n",
    "        final_count = cleaned_df[race_col].sum()\n",
    "        final_pct = (final_count / len(cleaned_df)) * 100\n",
    "        initial_count, initial_pct = initial_dist[race_name]\n",
    "        change_pct = final_pct - initial_pct\n",
    "        print(f\"{race_name:15} {initial_count:5d} ({initial_pct:5.1f}%)    {final_count:5d} ({final_pct:5.1f}%)    {change_pct:+5.1f}%\")\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "# Assuming your DataFrame is called 'df'\n",
    "df = remove_race_outliers(df, percentage=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Race Distribution:\n",
      "Bengal            238 cats (  7.8%)\n",
      "Birman            192 cats (  6.3%)\n",
      "British Shorthair   166 cats (  5.4%)\n",
      "Chartreux          31 cats (  1.0%)\n",
      "European         1018 cats ( 33.3%)\n",
      "Maine coon        197 cats (  6.4%)\n",
      "No breed          482 cats ( 15.8%)\n",
      "Other             135 cats (  4.4%)\n",
      "Persian           192 cats (  6.3%)\n",
      "Ragdoll           216 cats (  7.1%)\n",
      "Savannah           26 cats (  0.9%)\n",
      "Siamese            58 cats (  1.9%)\n",
      "Sphynx             76 cats (  2.5%)\n",
      "Turkish angora     28 cats (  0.9%)\n",
      "\n",
      "Removed 158 records (5.2% of total)\n",
      "\n",
      "Final Race Distribution:\n",
      "Race            Initial Count (%)    Final Count (%)    Change\n",
      "-----------------------------------------------------------------\n",
      "Bengal            238 (  7.8%)      226 (  7.8%)     +0.0%\n",
      "Birman            192 (  6.3%)      182 (  6.3%)     -0.0%\n",
      "British Shorthair   166 (  5.4%)      157 (  5.4%)     -0.0%\n",
      "Chartreux          31 (  1.0%)       29 (  1.0%)     -0.0%\n",
      "European         1018 ( 33.3%)      967 ( 33.4%)     +0.1%\n",
      "Maine coon        197 (  6.4%)      187 (  6.5%)     +0.0%\n",
      "No breed          482 ( 15.8%)      457 ( 15.8%)     -0.0%\n",
      "Other             135 (  4.4%)      128 (  4.4%)     -0.0%\n",
      "Persian           192 (  6.3%)      182 (  6.3%)     -0.0%\n",
      "Ragdoll           216 (  7.1%)      205 (  7.1%)     +0.0%\n",
      "Savannah           26 (  0.9%)       24 (  0.8%)     -0.0%\n",
      "Siamese            58 (  1.9%)       55 (  1.9%)     -0.0%\n",
      "Sphynx             76 (  2.5%)       72 (  2.5%)     -0.0%\n",
      "Turkish angora     28 (  0.9%)       26 (  0.9%)     -0.0%\n"
     ]
    }
   ],
   "execution_count": 157
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Training a model using a dataset wich is balanced using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:59:09.565407Z",
     "start_time": "2024-12-26T16:59:09.238701Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "def balance_dataset_smote(df, race_counts_dict):\n",
    "    \"\"\"\n",
    "    Balance the dataset using SMOTE according to custom counts for each race.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "    race_counts_dict (dict): Dictionary with races as keys and desired counts as values\n",
    "                            Example: {'European': 500, 'Sphynx': 100, ...}\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Balanced dataset with specified counts for each race\n",
    "    \"\"\"\n",
    "    # Get all race columns\n",
    "    race_columns = [col for col in df.columns if col.startswith('Race_')]\n",
    "    \n",
    "    # Create feature matrix X (excluding race columns)\n",
    "    feature_columns = [col for col in df.columns if not col.startswith('Race_')]\n",
    "    X = df[feature_columns]\n",
    "    \n",
    "    # Process each race separately\n",
    "    balanced_dfs = []\n",
    "    \n",
    "    for race in race_counts_dict.keys():\n",
    "        race_col = f'Race_{race}'\n",
    "        \n",
    "        if race_col not in df.columns:\n",
    "            print(f\"Warning: {race} not found in dataset. Skipping...\")\n",
    "            continue\n",
    "            \n",
    "        # Create binary classification problem for current race\n",
    "        y = df[race_col].astype(int)\n",
    "        \n",
    "        # Calculate sampling strategy\n",
    "        current_counts = Counter(y)\n",
    "        desired_count = race_counts_dict[race]\n",
    "        \n",
    "        # Only apply SMOTE if we need more samples and have at least one positive sample\n",
    "        if desired_count > current_counts[1] and current_counts[1] > 0:\n",
    "            # Calculate ratio to achieve desired count\n",
    "            sampling_strategy = {1: desired_count}\n",
    "            \n",
    "            try:\n",
    "                # Apply SMOTE\n",
    "                smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "                X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "                \n",
    "                # Create temporary DataFrame with features\n",
    "                temp_df = pd.DataFrame(X_resampled, columns=feature_columns)\n",
    "                \n",
    "                # Add race columns (all False by default)\n",
    "                for rc in race_columns:\n",
    "                    temp_df[rc] = False\n",
    "                \n",
    "                # Set current race to True where y_resampled is 1\n",
    "                temp_df[race_col] = y_resampled == 1\n",
    "                \n",
    "                # Only keep positive samples\n",
    "                temp_df = temp_df[temp_df[race_col]]\n",
    "                \n",
    "            except ValueError as e:\n",
    "                print(f\"Warning: SMOTE failed for {race}, using random oversampling instead\")\n",
    "                # Fallback to random oversampling\n",
    "                temp_df = df[df[race_col]].sample(n=desired_count, replace=True, random_state=42)\n",
    "                \n",
    "        else:\n",
    "            # If we want fewer samples, randomly select without replacement\n",
    "            n_samples = min(desired_count, current_counts[1])\n",
    "            temp_df = df[df[race_col]].sample(n=n_samples, random_state=42)\n",
    "        \n",
    "        balanced_dfs.append(temp_df)\n",
    "    \n",
    "    # Combine all balanced samples\n",
    "    balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    \n",
    "    # Print summary of new race distribution\n",
    "    print(\"\\nNew race distribution:\")\n",
    "    for race in race_counts_dict.keys():\n",
    "        race_col = f'Race_{race}'\n",
    "        if race_col in balanced_df.columns:\n",
    "            count = balanced_df[race_col].sum()\n",
    "            print(f\"{race}: {int(count)}\")\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "# Example usage:\n",
    "# Define your desired counts for each race\n",
    "desired_counts = {\n",
    "    'European': 800,\n",
    "    'No breed': 600,\n",
    "    'Bengal': 500,\n",
    "    'Ragdoll': 400,\n",
    "    'Maine coon': 350,\n",
    "    'Birman': 300,\n",
    "    'Persian': 300,\n",
    "    'British Shorthair': 300,\n",
    "    'Other': 300,\n",
    "    'Sphynx': 150,\n",
    "    'Siamese': 100,\n",
    "    'Chartreux': 60,\n",
    "    'Turkish angora': 60,\n",
    "    'Savannah': 50\n",
    "}\n",
    "\n",
    "# Balance the dataset\n",
    "df = balance_dataset_smote(df, desired_counts)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New race distribution:\n",
      "European: 800\n",
      "No breed: 600\n",
      "Bengal: 500\n",
      "Ragdoll: 400\n",
      "Maine coon: 350\n",
      "Birman: 300\n",
      "Persian: 300\n",
      "British Shorthair: 300\n",
      "Other: 300\n",
      "Sphynx: 150\n",
      "Siamese: 100\n",
      "Chartreux: 60\n",
      "Turkish angora: 60\n",
      "Savannah: 50\n"
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_excel('Dataset.xlsx')\n",
    "\n",
    "print(df.info())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T17:00:58.582314Z",
     "start_time": "2024-12-26T17:00:53.288121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "df.to_excel('Dataset_Preprocessed.xlsx', index=False)"
   ],
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:59:12.508630Z",
     "start_time": "2024-12-26T16:59:09.723578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "best_model, accuracies, histories, scaler = train(df)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/6\n",
      "Epoch 5/1000\n",
      "Train Loss: 1.8295, Train Acc: 0.4051\n",
      "Val Loss: 1.6840, Val Acc: 0.4551\n",
      "Epoch 10/1000\n",
      "Train Loss: 1.3956, Train Acc: 0.5441\n",
      "Val Loss: 1.2978, Val Acc: 0.5997\n",
      "Epoch 15/1000\n",
      "Train Loss: 1.1857, Train Acc: 0.5974\n",
      "Val Loss: 1.0981, Val Acc: 0.6180\n",
      "Epoch 20/1000\n",
      "Train Loss: 1.0556, Train Acc: 0.6287\n",
      "Val Loss: 0.9818, Val Acc: 0.6489\n",
      "Epoch 25/1000\n",
      "Train Loss: 0.9726, Train Acc: 0.6376\n",
      "Val Loss: 0.9052, Val Acc: 0.6713\n",
      "Epoch 30/1000\n",
      "Train Loss: 0.9171, Train Acc: 0.6540\n",
      "Val Loss: 0.8544, Val Acc: 0.6713\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[160], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m best_model, accuracies, histories, scaler \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[151], line 276\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(data):\n\u001B[0;32m    275\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m prepare_data(data)\n\u001B[1;32m--> 276\u001B[0m     best_model, fold_results, histories, scaler \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_and_evaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    277\u001B[0m     save_model(best_model, scaler)\n\u001B[0;32m    278\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m best_model, fold_results, histories, scaler\n",
      "Cell \u001B[1;32mIn[151], line 192\u001B[0m, in \u001B[0;36mtrain_and_evaluate_model\u001B[1;34m(X, y, n_splits, epochs, batch_size, learning_rate, patience)\u001B[0m\n\u001B[0;32m    189\u001B[0m X_batch \u001B[38;5;241m=\u001B[39m X_train_shuffled[i:i\u001B[38;5;241m+\u001B[39mbatch_size]\n\u001B[0;32m    190\u001B[0m y_batch \u001B[38;5;241m=\u001B[39m y_train_shuffled[i:i\u001B[38;5;241m+\u001B[39mbatch_size]\n\u001B[1;32m--> 192\u001B[0m loss, predictions \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    193\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mcompute_accuracy(predictions, y_batch)\n\u001B[0;32m    195\u001B[0m epoch_train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n",
      "Cell \u001B[1;32mIn[151], line 103\u001B[0m, in \u001B[0;36mNeuralNetwork.train_step\u001B[1;34m(self, X_batch, y_batch, learning_rate)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, X_batch, y_batch, learning_rate):\n\u001B[1;32m--> 103\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    104\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackward(X_batch, y_batch, learning_rate)\n\u001B[0;32m    105\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss(predictions, y_batch)\n",
      "Cell \u001B[1;32mIn[151], line 94\u001B[0m, in \u001B[0;36mNeuralNetwork.forward\u001B[1;34m(self, X, training)\u001B[0m\n\u001B[0;32m     92\u001B[0m current_output \u001B[38;5;241m=\u001B[39m X\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m---> 94\u001B[0m     current_output \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcurrent_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m current_output\n",
      "Cell \u001B[1;32mIn[151], line 54\u001B[0m, in \u001B[0;36mLayer.forward\u001B[1;34m(self, input_data, training)\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput \u001B[38;5;241m=\u001B[39m exp_values \u001B[38;5;241m/\u001B[39m np\u001B[38;5;241m.\u001B[39msum(exp_values, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msoftmax\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m---> 54\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput\n",
      "Cell \u001B[1;32mIn[151], line 15\u001B[0m, in \u001B[0;36mDropout.forward\u001B[1;34m(self, input_data, training)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_data, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m training:\n\u001B[1;32m---> 15\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmask \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbinomial\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout_rate)\n\u001B[0;32m     16\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m input_data \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmask\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m input_data\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 160
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
