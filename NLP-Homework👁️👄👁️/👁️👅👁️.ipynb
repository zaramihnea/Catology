{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-24T21:47:29.358234Z",
     "start_time": "2024-12-24T21:47:28.541700Z"
    }
   },
   "source": [
    "import nltk\n",
    "from langdetect import detect, LangDetectException\n",
    "import os\n",
    "\n",
    "def download_nltk_resources():\n",
    "    \"\"\"Download required NLTK resources if not already present.\"\"\"\n",
    "    try:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading NLTK resources: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"\n",
    "    Read content from a text file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the text file\n",
    "    \n",
    "    Returns:\n",
    "        str: Content of the file\n",
    "        None: If there's an error reading the file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detect the language of the given text using langdetect.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (language_code, confidence_score)\n",
    "        None: If detection fails\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Detect language\n",
    "        lang_code = detect(text)\n",
    "        \n",
    "        # Map language codes to full names\n",
    "        language_names = {\n",
    "            'en': 'English',\n",
    "            'es': 'Spanish',\n",
    "            'fr': 'French',\n",
    "            'de': 'German',\n",
    "            'it': 'Italian',\n",
    "            'pt': 'Portuguese',\n",
    "            'nl': 'Dutch',\n",
    "            'ru': 'Russian',\n",
    "            'ar': 'Arabic',\n",
    "            'ja': 'Japanese',\n",
    "            'ko': 'Korean',\n",
    "            'ro': 'Romanian',\n",
    "            'zh-cn': 'Chinese (Simplified)',\n",
    "            'zh-tw': 'Chinese (Traditional)',\n",
    "        }\n",
    "        \n",
    "        detected_language = language_names.get(lang_code, lang_code)\n",
    "        return detected_language\n",
    "        \n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Initialize NLTK resources\n",
    "    if not download_nltk_resources():\n",
    "        return\n",
    "    \n",
    "    file_path = 'sample_phrase.txt'\n",
    "    \n",
    "    # Read the file\n",
    "    text = read_file(file_path)\n",
    "    if text is None:\n",
    "        return\n",
    "    \n",
    "    # Detect language\n",
    "    detected_language = detect_language(text)\n",
    "    \n",
    "    if detected_language:\n",
    "        print(f\"\\nInput text: {text}\")\n",
    "        print(f\"Detected language: {detected_language}\")\n",
    "    else:\n",
    "        print(\"Could not detect the language of the text.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input text: I like cats. I also like dogs. What i do not like is when people sit.\n",
      "Detected language: English\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T21:54:45.611191Z",
     "start_time": "2024-12-24T21:54:45.589330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import statistics\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class RobustStyleMetrics:\n",
    "    def __init__(self, text: str):\n",
    "        \"\"\"\n",
    "        Initialize the stylometry analyzer with text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to analyze\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.words = self._tokenize_words()\n",
    "        self.chars = list(text)\n",
    "        \n",
    "    def _tokenize_words(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize text into words using a simple but robust method.\n",
    "        Handles Romanian diacritics properly.\n",
    "        \"\"\"\n",
    "        # Clean the text and split on whitespace and punctuation\n",
    "        # This regex preserves Romanian characters\n",
    "        text = self.text.lower()\n",
    "        words = re.findall(r'[a-zăâîșțA-ZĂÂÎȘȚ]+', text)\n",
    "        return words\n",
    "\n",
    "    def word_length_stats(self) -> Dict:\n",
    "        \"\"\"Calculate statistics about word lengths.\"\"\"\n",
    "        word_lengths = [len(word) for word in self.words]\n",
    "        if not word_lengths:\n",
    "            return {\"average\": 0, \"median\": 0, \"std_dev\": 0}\n",
    "        \n",
    "        return {\n",
    "            \"average\": statistics.mean(word_lengths),\n",
    "            \"median\": statistics.median(word_lengths),\n",
    "            \"std_dev\": statistics.stdev(word_lengths) if len(word_lengths) > 1 else 0\n",
    "        }\n",
    "\n",
    "    def word_frequency(self, top_n: int = 10) -> List[Tuple[str, int]]:\n",
    "        \"\"\"Get the frequency of words.\"\"\"\n",
    "        return Counter(self.words).most_common(top_n)\n",
    "\n",
    "    def char_frequency(self) -> Dict[str, float]:\n",
    "        \"\"\"Calculate character frequency as percentages.\"\"\"\n",
    "        char_count = Counter(self.chars)\n",
    "        total_chars = len(self.chars)\n",
    "        return {char: (count/total_chars)*100 \n",
    "                for char, count in char_count.items()\n",
    "                if not char.isspace()}\n",
    "\n",
    "    def basic_stats(self) -> Dict:\n",
    "        \"\"\"Calculate basic text statistics.\"\"\"\n",
    "        # Simple sentence splitting on .!?\n",
    "        sentences = re.split('[.!?]+', self.text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        return {\n",
    "            \"total_characters\": len(self.chars),\n",
    "            \"total_words\": len(self.words),\n",
    "            \"avg_word_length\": sum(len(word) for word in self.words) / len(self.words) if self.words else 0,\n",
    "            \"sentences\": len(sentences),\n",
    "            \"avg_words_per_sentence\": len(self.words) / len(sentences) if sentences else 0\n",
    "        }\n",
    "\n",
    "    def generate_report(self) -> Dict:\n",
    "        \"\"\"Generate a complete stylometric report.\"\"\"\n",
    "        return {\n",
    "            \"basic_statistics\": self.basic_stats(),\n",
    "            \"word_length_statistics\": self.word_length_stats(),\n",
    "            \"most_common_words\": self.word_frequency(10),\n",
    "            \"character_frequencies\": self.char_frequency()\n",
    "        }\n",
    "\n",
    "def print_stylometry_report(text: str):\n",
    "    \"\"\"\n",
    "    Print a formatted stylometry report for the given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to analyze\n",
    "    \"\"\"\n",
    "    analyzer = RobustStyleMetrics(text)\n",
    "    report = analyzer.generate_report()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RAPORT DE ANALIZĂ STILOMETRICĂ\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    print(\"STATISTICI DE BAZĂ:\")\n",
    "    print(\"-\" * 20)\n",
    "    stats_translation = {\n",
    "        \"total_characters\": \"Total caractere\",\n",
    "        \"total_words\": \"Total cuvinte\",\n",
    "        \"avg_word_length\": \"Lungime medie cuvânt\",\n",
    "        \"sentences\": \"Număr propoziții\",\n",
    "        \"avg_words_per_sentence\": \"Media cuvintelor per propoziție\"\n",
    "    }\n",
    "    \n",
    "    for key, value in report['basic_statistics'].items():\n",
    "        print(f\"{stats_translation[key]}: {value:.2f}\" if isinstance(value, float) \n",
    "              else f\"{stats_translation[key]}: {value}\")\n",
    "    \n",
    "    print(\"\\nSTATISTICI LUNGIME CUVINTE:\")\n",
    "    print(\"-\" * 20)\n",
    "    length_stats_translation = {\n",
    "        \"average\": \"Medie\",\n",
    "        \"median\": \"Mediană\",\n",
    "        \"std_dev\": \"Deviație standard\"\n",
    "    }\n",
    "    for key, value in report['word_length_statistics'].items():\n",
    "        print(f\"{length_stats_translation[key]}: {value:.2f}\")\n",
    "    \n",
    "    print(\"\\nCELE MAI FRECVENTE CUVINTE:\")\n",
    "    print(\"-\" * 20)\n",
    "    for word, freq in report['most_common_words']:\n",
    "        print(f\"'{word}': {freq} ori\")\n",
    "    \n",
    "    print(\"\\nFRECVENȚA CARACTERELOR (%):\")\n",
    "    print(\"-\" * 20)\n",
    "    for char, freq in sorted(report['character_frequencies'].items()):\n",
    "        if freq >= 1.0:  # Show only chars with >= 1% frequency\n",
    "            print(f\"'{char}': {freq:.2f}%\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    text = read_file('sample_phrase.txt')\n",
    "    print_stylometry_report(text)"
   ],
   "id": "9a73fe3c72fce96e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RAPORT DE ANALIZĂ STILOMETRICĂ\n",
      "==================================================\n",
      "\n",
      "STATISTICI DE BAZĂ:\n",
      "--------------------\n",
      "Total caractere: 69\n",
      "Total cuvinte: 16\n",
      "Lungime medie cuvânt: 3.19\n",
      "Număr propoziții: 3\n",
      "Media cuvintelor per propoziție: 5.33\n",
      "\n",
      "STATISTICI LUNGIME CUVINTE:\n",
      "--------------------\n",
      "Medie: 3.19\n",
      "Mediană: 4.00\n",
      "Deviație standard: 1.42\n",
      "\n",
      "CELE MAI FRECVENTE CUVINTE:\n",
      "--------------------\n",
      "'i': 3 ori\n",
      "'like': 3 ori\n",
      "'cats': 1 ori\n",
      "'also': 1 ori\n",
      "'dogs': 1 ori\n",
      "'what': 1 ori\n",
      "'do': 1 ori\n",
      "'not': 1 ori\n",
      "'is': 1 ori\n",
      "'when': 1 ori\n",
      "\n",
      "FRECVENȚA CARACTERELOR (%):\n",
      "--------------------\n",
      "'.': 4.35%\n",
      "'I': 2.90%\n",
      "'W': 1.45%\n",
      "'a': 4.35%\n",
      "'c': 1.45%\n",
      "'d': 2.90%\n",
      "'e': 8.70%\n",
      "'g': 1.45%\n",
      "'h': 2.90%\n",
      "'i': 8.70%\n",
      "'k': 4.35%\n",
      "'l': 7.25%\n",
      "'n': 2.90%\n",
      "'o': 7.25%\n",
      "'p': 2.90%\n",
      "'s': 7.25%\n",
      "'t': 5.80%\n",
      "'w': 1.45%\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T23:38:01.873118Z",
     "start_time": "2024-12-24T23:38:01.838340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import random\n",
    "import logging\n",
    "\n",
    "def download_required_resources():\n",
    "    \"\"\"Download required NLTK resources if not already present.\"\"\"\n",
    "    # Temporarily disable NLTK download messages\n",
    "    logging.getLogger('nltk.downloader').disabled = True\n",
    "    \n",
    "    resources = ['punkt', 'averaged_perceptron_tagger', 'wordnet']\n",
    "    for resource in resources:\n",
    "        try:\n",
    "            nltk.data.find(f'tokenizers/{resource}' if resource == 'punkt' \n",
    "                          else f'taggers/{resource}' if resource == 'averaged_perceptron_tagger'\n",
    "                          else f'corpora/{resource}')\n",
    "        except LookupError:\n",
    "            nltk.download(resource, quiet=True)\n",
    "    \n",
    "    # Re-enable NLTK download messages for future operations if needed\n",
    "    logging.getLogger('nltk.downloader').disabled = False\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert POS tag to WordNet POS tag.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_alternatives(word, pos):\n",
    "    \"\"\"Get synonyms, hypernyms, and negated antonyms for a word.\"\"\"\n",
    "    alternatives = []\n",
    "    \n",
    "    if not pos:\n",
    "        return alternatives\n",
    "    \n",
    "    # Get synsets for the word with the correct POS\n",
    "    synsets = wn.synsets(word, pos=pos)\n",
    "    \n",
    "    if not synsets:\n",
    "        return alternatives\n",
    "    \n",
    "    # Get synonyms\n",
    "    for synset in synsets:\n",
    "        for lemma in synset.lemmas():\n",
    "            if lemma.name() != word and '_' not in lemma.name():\n",
    "                alternatives.append(('synonym', lemma.name()))\n",
    "    \n",
    "    # Get hypernyms\n",
    "    for synset in synsets:\n",
    "        hypernyms = synset.hypernyms()\n",
    "        for hypernym in hypernyms:\n",
    "            for lemma in hypernym.lemmas():\n",
    "                if lemma.name() != word and '_' not in lemma.name():\n",
    "                    alternatives.append(('hypernym', lemma.name()))\n",
    "    \n",
    "    # Get antonyms\n",
    "    for synset in synsets:\n",
    "        for lemma in synset.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                for antonym in lemma.antonyms():\n",
    "                    if '_' not in antonym.name():\n",
    "                        alternatives.append(('antonym', f\"not {antonym.name()}\"))\n",
    "    \n",
    "    return list(set(alternatives))\n",
    "\n",
    "def generate_variations(phrase, variation_percentage=0.4):\n",
    "    \"\"\"Generate variations of the input phrase by replacing words.\"\"\"\n",
    "    # Download required resources silently\n",
    "    download_required_resources()\n",
    "    \n",
    "    # Tokenize and POS tag the phrase\n",
    "    tokens = word_tokenize(phrase)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Calculate number of words to replace\n",
    "    num_replacements = max(1, int(len(tokens) * variation_percentage))\n",
    "    \n",
    "    variations = []\n",
    "    for _ in range(5):  # Generate 5 different variations\n",
    "        # Make a copy of the tokens\n",
    "        new_tokens = tokens.copy()\n",
    "        \n",
    "        # Randomly select positions to replace\n",
    "        positions_to_replace = random.sample(range(len(tokens)), num_replacements)\n",
    "        \n",
    "        for pos in positions_to_replace:\n",
    "            word = tokens[pos]\n",
    "            tag = tagged[pos][1]\n",
    "            wordnet_pos = get_wordnet_pos(tag)\n",
    "            \n",
    "            alternatives = get_alternatives(word.lower(), wordnet_pos)\n",
    "            if alternatives:\n",
    "                replacement_type, replacement = random.choice(alternatives)\n",
    "                # Preserve original capitalization\n",
    "                if word[0].isupper():\n",
    "                    replacement = replacement.capitalize()\n",
    "                new_tokens[pos] = replacement\n",
    "        \n",
    "        variation = ' '.join(new_tokens)\n",
    "        if variation != phrase:  # Only add if different from original\n",
    "            variations.append(variation)\n",
    "    \n",
    "    return variations\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    test_phrases = [\n",
    "        \"The old house stands far from the busy road\",\n",
    "        \"The quick brown fox jumps over the lazy dog\",\n",
    "        \"She walks slowly through the beautiful garden\"\n",
    "    ]\n",
    "    \n",
    "    for phrase in test_phrases:\n",
    "        print(f\"\\nOriginal phrase: {phrase}\")\n",
    "        print(\"Variations:\")\n",
    "        variations = generate_variations(phrase)\n",
    "        for i, variation in enumerate(variations, 1):\n",
    "            print(f\"{i}. {variation}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "45682e2797e84471",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original phrase: The old house stands far from the busy road\n",
      "Variations:\n",
      "1. The quondam house rest far from the officious road\n",
      "2. The old house stands far from the fussy way\n",
      "3. The old house stands far from the busybodied route\n",
      "4. The older house stands far from the busy means\n",
      "5. The old community stands far from the busy route\n",
      "\n",
      "Original phrase: The quick brown fox jumps over the lazy dog\n",
      "Variations:\n",
      "1. The quick brown trickster jump-start over the lazy cuss\n",
      "2. The speedy brown fox jumps over the slothful dog\n",
      "3. The speedy brown fox skip over the lazy fellow\n",
      "4. The quick brownness fox jumps over the lazy hotdog\n",
      "5. The quick brown canine jumps over the faineant canine\n",
      "\n",
      "Original phrase: She walks slowly through the beautiful garden\n",
      "Variations:\n",
      "1. She walks slowly through the beautiful vegetation\n",
      "2. She walks slowly through the not ugly garden\n",
      "3. She score slowly through the beautiful yard\n",
      "4. She walks not quickly through the beautiful garden\n",
      "5. She walks tardily through the beautiful garden\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T17:00:32.231300Z",
     "start_time": "2024-12-25T17:00:18.176731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yake  # for keyword extraction\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def extract_and_generate_phrases(text, num_keywords=5):\n",
    "    # Initialize keyword extractor\n",
    "    kw_extractor = yake.KeywordExtractor(\n",
    "        lan=\"en\",\n",
    "        n=1,  # unigrams\n",
    "        dedupLim=0.9,\n",
    "        top=num_keywords,\n",
    "        features=None\n",
    "    )\n",
    "    \n",
    "    # Extract keywords\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    \n",
    "    # Initialize local LLM (BLOOMZ-560M as an example - relatively small but effective)\n",
    "    model_name = \"bigscience/bloomz-560m\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for keyword, score in keywords:\n",
    "        # Create prompt for the LLM\n",
    "        prompt = f\"Context: {text}\\n\\nTo explain the word {keyword} in simple terms:\"\n",
    "        \n",
    "        # Generate response using the local LLM\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=150,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_phrase = generated_text.split(\"in simple terms:\")[1].strip()\n",
    "        \n",
    "        results.append({\n",
    "            'keyword': keyword,\n",
    "            'original_context': text,\n",
    "            'generated_phrase': generated_phrase\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    Artificial intelligence is revolutionizing healthcare through improved diagnosis \n",
    "    and treatment planning. Machine learning algorithms can analyze medical images \n",
    "    and patient data to detect patterns that humans might miss.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = extract_and_generate_phrases(sample_text)\n",
    "    \n",
    "    # Print results\n",
    "    for result in results:\n",
    "        print(f\"\\nKeyword: {result['keyword']}\")\n",
    "        print(f\"Generated Phrase: {result['generated_phrase']}\")\n"
   ],
   "id": "8c8ed6b6ab92e8de",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mihai\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mihai\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keyword: Artificial\n",
      "Generated Phrase: Artificial intelligence is the ability to learn from the data and make intelligent decisions.\n",
      "\n",
      "Keyword: planning\n",
      "Generated Phrase: The process of planning a medical procedure.\n",
      "\n",
      "Keyword: intelligence\n",
      "Generated Phrase: Intelligence is the ability to reason, understand, and act on information.\n",
      "\n",
      "Keyword: revolutionizing\n",
      "Generated Phrase: AI revolutionizes healthcare through improved diagnosis and treatment planning. Machine learning algorithms can analyze medical images and patient data to detect patterns that humans might miss.\n",
      "\n",
      "Keyword: healthcare\n",
      "Generated Phrase: healthcare is the science and technology of medicine and health care.\n"
     ]
    }
   ],
   "execution_count": 76
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
