{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_auc: 0.9997\n",
    "accuracy: 0.9817\n",
    "\n",
    "Test Metrics:\n",
    "precision_macro: 0.7433\n",
    "precision_micro: 0.7405\n",
    "recall_macro: 0.7277\n",
    "recall_micro: 0.7255\n",
    "f1_macro: 0.7327\n",
    "f1_micro: 0.7329\n",
    "roc_auc: 0.9542\n",
    "accuracy: 0.7255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8790febe22fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# First, let's prepare our data\n",
    "# Get feature columns (excluding the Race_ columns)\n",
    "race_columns = [col for col in train_df.columns if col.startswith('Race_')]\n",
    "feature_columns = [col for col in train_df.columns if not col.startswith('Race_')]\n",
    "\n",
    "# Handle missing values in Age column\n",
    "train_df['Age'].fillna(train_df['Age'].mean(), inplace=True)\n",
    "\n",
    "# Prepare X and y\n",
    "X = train_df[feature_columns].values\n",
    "y = train_df[race_columns].values\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def create_model(input_dim, l2_lambda=0.0001):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(len(race_columns), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# [Rest of the code remains exactly the same]\n",
    "def plot_confusion_matrices(y_true, y_pred, class_names):\n",
    "    \"\"\"Plot confusion matrices for each class\"\"\"\n",
    "    fig, axes = plt.subplots(2, (len(class_names) + 1) // 2, figsize=(15, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        cm = confusion_matrix(y_true[:, idx], (y_pred[:, idx] > 0.5).astype(int))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=axes[idx])\n",
    "        axes[idx].set_title(f'Confusion Matrix - {class_name}')\n",
    "        axes[idx].set_xlabel('Predicted')\n",
    "        axes[idx].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def calculate_metrics(y_true, y_pred_proba):\n",
    "    \"\"\"Calculate comprehensive metrics for multi-label classification\"\"\"\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro'),\n",
    "        'precision_micro': precision_score(y_true, y_pred, average='micro'),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro'),\n",
    "        'recall_micro': recall_score(y_true, y_pred, average='micro'),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
    "        'f1_micro': f1_score(y_true, y_pred, average='micro'),\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_proba, average='macro'),\n",
    "        'accuracy': (y_true == y_pred).all(axis=1).mean()\n",
    "    }\n",
    "    \n",
    "    # Per-class metrics\n",
    "    class_metrics = {\n",
    "        'precision_per_class': precision_score(y_true, y_pred, average=None),\n",
    "        'recall_per_class': recall_score(y_true, y_pred, average=None),\n",
    "        'f1_per_class': f1_score(y_true, y_pred, average=None),\n",
    "        'roc_auc_per_class': roc_auc_score(y_true, y_pred_proba, average=None)\n",
    "    }\n",
    "    \n",
    "    return metrics, class_metrics\n",
    "\n",
    "# Perform 10-fold cross validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "all_metrics = []\n",
    "all_class_metrics = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\nFold {fold + 1}/10\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Scale features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_model(X_train.shape[1])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_metrics, train_class_metrics = calculate_metrics(y_train, y_pred_train)\n",
    "    test_metrics, test_class_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # Store metrics\n",
    "    all_metrics.append({\n",
    "        'fold': fold + 1,\n",
    "        'train': train_metrics,\n",
    "        'test': test_metrics\n",
    "    })\n",
    "    \n",
    "    all_class_metrics.append({\n",
    "        'fold': fold + 1,\n",
    "        'train': train_class_metrics,\n",
    "        'test': test_class_metrics\n",
    "    })\n",
    "    \n",
    "    # Print current fold metrics\n",
    "    print(\"\\nTrain Metrics:\")\n",
    "    for metric_name, value in train_metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nTest Metrics:\")\n",
    "    for metric_name, value in test_metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrices for this fold\n",
    "    plot_confusion_matrices(y_test, y_pred_test, race_columns)\n",
    "    plt.show()\n",
    "\n",
    "# Calculate and print final average results\n",
    "print(\"\\nFinal Average Results:\")\n",
    "print(\"\\nOverall Metrics:\")\n",
    "for metric_name in all_metrics[0]['test'].keys():\n",
    "    test_values = [fold['test'][metric_name] for fold in all_metrics]\n",
    "    mean_value = np.mean(test_values)\n",
    "    std_value = np.std(test_values)\n",
    "    print(f\"{metric_name}: {mean_value:.4f} ± {std_value:.4f}\")\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "for class_idx, class_name in enumerate(race_columns):\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    for metric_name in ['precision_per_class', 'recall_per_class', 'f1_per_class', 'roc_auc_per_class']:\n",
    "        test_values = [fold['test'][metric_name][class_idx] for fold in all_class_metrics]\n",
    "        mean_value = np.mean(test_values)\n",
    "        std_value = np.std(test_values)\n",
    "        print(f\"{metric_name}: {mean_value:.4f} ± {std_value:.4f}\")\n",
    "\n",
    "# Plot learning curves from the last fold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ac63ced65c9f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "\n",
    "# First, let's prepare our data\n",
    "# Get feature columns (excluding the Race_ columns)\n",
    "race_columns = [col for col in train_df.columns if col.startswith('Race_')]\n",
    "feature_columns = [col for col in train_df.columns if not col.startswith('Race_')]\n",
    "\n",
    "# Handle missing values in Age column\n",
    "train_df['Age'].fillna(train_df['Age'].mean(), inplace=True)\n",
    "\n",
    "# Prepare X and y\n",
    "X = train_df[feature_columns].values\n",
    "y = train_df[race_columns].values\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def create_model(input_dim, l2_lambda=0.0001):\n",
    "    initial_learning_rate = 0.001\n",
    "    decay_steps = 1000\n",
    "    decay_rate = 0.9\n",
    "    learning_rate_schedule = ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        staircase=True\n",
    "    )\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim,\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda)),\n",
    "        Dense(128, activation='relu',\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda)),\n",
    "        Dense(128, activation='relu',\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda)),\n",
    "        Dense(64, activation='relu',\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda)),\n",
    "        Dense(64, activation='relu',\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda)),\n",
    "        Dense(len(race_columns), activation='softmax',\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda))\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate_schedule),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# [Rest of the code remains exactly the same]\n",
    "def plot_confusion_matrices(y_true, y_pred, class_names):\n",
    "    \"\"\"Plot confusion matrices for each class\"\"\"\n",
    "    fig, axes = plt.subplots(2, (len(class_names) + 1) // 2, figsize=(15, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        cm = confusion_matrix(y_true[:, idx], (y_pred[:, idx] > 0.5).astype(int))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=axes[idx])\n",
    "        axes[idx].set_title(f'Confusion Matrix - {class_name}')\n",
    "        axes[idx].set_xlabel('Predicted')\n",
    "        axes[idx].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def calculate_metrics(y_true, y_pred_proba):\n",
    "    \"\"\"Calculate comprehensive metrics for multi-label classification\"\"\"\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro'),\n",
    "        'precision_micro': precision_score(y_true, y_pred, average='micro'),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro'),\n",
    "        'recall_micro': recall_score(y_true, y_pred, average='micro'),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
    "        'f1_micro': f1_score(y_true, y_pred, average='micro'),\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_proba, average='macro'),\n",
    "        'accuracy': (y_true == y_pred).all(axis=1).mean()\n",
    "    }\n",
    "    \n",
    "    # Per-class metrics\n",
    "    class_metrics = {\n",
    "        'precision_per_class': precision_score(y_true, y_pred, average=None),\n",
    "        'recall_per_class': recall_score(y_true, y_pred, average=None),\n",
    "        'f1_per_class': f1_score(y_true, y_pred, average=None),\n",
    "        'roc_auc_per_class': roc_auc_score(y_true, y_pred_proba, average=None)\n",
    "    }\n",
    "    \n",
    "    return metrics, class_metrics\n",
    "\n",
    "# Perform 10-fold cross validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "all_metrics = []\n",
    "all_class_metrics = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\nFold {fold + 1}/10\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Scale features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_model(X_train.shape[1])\n",
    "    \n",
    "    # Define callbacks    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=f'model_checkpoints/model_fold_{fold+1}.keras',  # Changed from .h5 to .keras\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,     # Only save when the monitored metric is improved\n",
    "        mode='max',              # We want to minimize the validation loss\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model with callbacks\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=100,              # Maximum number of epochs\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        callbacks=[model_checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Load the best model for this fold\n",
    "    best_model = load_model(f'model_checkpoints/model_fold_{fold+1}.keras')\n",
    "    \n",
    "    # Get predictions using the best model\n",
    "    y_pred_train = best_model.predict(X_train_scaled)\n",
    "    y_pred_test = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_metrics, train_class_metrics = calculate_metrics(y_train, y_pred_train)\n",
    "    test_metrics, test_class_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    \n",
    "    # Store metrics\n",
    "    all_metrics.append({\n",
    "        'fold': fold + 1,\n",
    "        'train': train_metrics,\n",
    "        'test': test_metrics\n",
    "    })\n",
    "    \n",
    "    all_class_metrics.append({\n",
    "        'fold': fold + 1,\n",
    "        'train': train_class_metrics,\n",
    "        'test': test_class_metrics\n",
    "    })\n",
    "    \n",
    "    # Print current fold metrics\n",
    "    print(\"\\nTrain Metrics:\")\n",
    "    for metric_name, value in train_metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nTest Metrics:\")\n",
    "    for metric_name, value in test_metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrices for this fold\n",
    "    plot_confusion_matrices(y_test, y_pred_test, race_columns)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot learning curves\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.axvline(x=np.argmin(history.history['val_loss']), color='r', linestyle='--', \n",
    "                label='Best Model')\n",
    "    plt.title('Model Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.axvline(x=np.argmin(history.history['val_loss']), color='r', linestyle='--', \n",
    "                label='Best Model')\n",
    "    plt.title('Model Accuracy Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Calculate and print final average results\n",
    "print(\"\\nFinal Average Results:\")\n",
    "print(\"\\nOverall Metrics:\")\n",
    "for metric_name in all_metrics[0]['test'].keys():\n",
    "    test_values = [fold['test'][metric_name] for fold in all_metrics]\n",
    "    mean_value = np.mean(test_values)\n",
    "    std_value = np.std(test_values)\n",
    "    print(f\"{metric_name}: {mean_value:.4f} ± {std_value:.4f}\")\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "for class_idx, class_name in enumerate(race_columns):\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    for metric_name in ['precision_per_class', 'recall_per_class', 'f1_per_class', 'roc_auc_per_class']:\n",
    "        test_values = [fold['test'][metric_name][class_idx] for fold in all_class_metrics]\n",
    "        mean_value = np.mean(test_values)\n",
    "        std_value = np.std(test_values)\n",
    "        print(f\"{metric_name}: {mean_value:.4f} ± {std_value:.4f}\")\n",
    "\n",
    "# Plot learning curves from the last fold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf822fa6576220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "\n",
    "# First, let's prepare our data\n",
    "# Get feature columns (excluding the Race_ columns)\n",
    "race_columns = [col for col in train_df.columns if col.startswith('Race_')]\n",
    "feature_columns = [col for col in train_df.columns if not col.startswith('Race_')]\n",
    "\n",
    "# Handle missing values in Age column\n",
    "train_df['Age'].fillna(train_df['Age'].mean(), inplace=True)\n",
    "\n",
    "# Prepare X and y\n",
    "X = train_df[feature_columns].values\n",
    "y = train_df[race_columns].values\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def create_model(input_dim, l2_lambda=0.0001):\n",
    "    initial_learning_rate = 0.001\n",
    "    decay_steps = 1000\n",
    "    decay_rate = 0.9\n",
    "    learning_rate_schedule = ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        staircase=True\n",
    "    )\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim,\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda)),\n",
    "        Dense(128, activation='relu',\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda)),\n",
    "        Dense(128, activation='relu',\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda)),\n",
    "        Dense(128, activation='relu',\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda)),\n",
    "        Dense(len(race_columns), activation='softmax',\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda))\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate_schedule),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# [Rest of the code remains exactly the same]\n",
    "def plot_confusion_matrices(y_true, y_pred, class_names):\n",
    "    \"\"\"Plot confusion matrices for each class\"\"\"\n",
    "    fig, axes = plt.subplots(2, (len(class_names) + 1) // 2, figsize=(15, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        cm = confusion_matrix(y_true[:, idx], (y_pred[:, idx] > 0.5).astype(int))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=axes[idx])\n",
    "        axes[idx].set_title(f'Confusion Matrix - {class_name}')\n",
    "        axes[idx].set_xlabel('Predicted')\n",
    "        axes[idx].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def calculate_metrics(y_true, y_pred_proba):\n",
    "    \"\"\"Calculate comprehensive metrics for multi-label classification\"\"\"\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro'),\n",
    "        'precision_micro': precision_score(y_true, y_pred, average='micro'),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro'),\n",
    "        'recall_micro': recall_score(y_true, y_pred, average='micro'),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
    "        'f1_micro': f1_score(y_true, y_pred, average='micro'),\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_proba, average='macro'),\n",
    "        'accuracy': (y_true == y_pred).all(axis=1).mean()\n",
    "    }\n",
    "    \n",
    "    # Per-class metrics\n",
    "    class_metrics = {\n",
    "        'precision_per_class': precision_score(y_true, y_pred, average=None),\n",
    "        'recall_per_class': recall_score(y_true, y_pred, average=None),\n",
    "        'f1_per_class': f1_score(y_true, y_pred, average=None),\n",
    "        'roc_auc_per_class': roc_auc_score(y_true, y_pred_proba, average=None)\n",
    "    }\n",
    "    \n",
    "    return metrics, class_metrics\n",
    "\n",
    "# Perform 10-fold cross validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "all_metrics = []\n",
    "all_class_metrics = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\nFold {fold + 1}/10\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Scale features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_model(X_train.shape[1])\n",
    "    \n",
    "    # Define callbacks    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=f'model_checkpoints/model_fold_{fold+1}.keras',  # Changed from .h5 to .keras\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,     # Only save when the monitored metric is improved\n",
    "        mode='max',              # We want to minimize the validation loss\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model with callbacks\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=100,              # Maximum number of epochs\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        callbacks=[model_checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Load the best model for this fold\n",
    "    best_model = load_model(f'model_checkpoints/model_fold_{fold+1}.keras')\n",
    "    \n",
    "    # Get predictions using the best model\n",
    "    y_pred_train = best_model.predict(X_train_scaled)\n",
    "    y_pred_test = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_metrics, train_class_metrics = calculate_metrics(y_train, y_pred_train)\n",
    "    test_metrics, test_class_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    \n",
    "    # Store metrics\n",
    "    all_metrics.append({\n",
    "        'fold': fold + 1,\n",
    "        'train': train_metrics,\n",
    "        'test': test_metrics\n",
    "    })\n",
    "    \n",
    "    all_class_metrics.append({\n",
    "        'fold': fold + 1,\n",
    "        'train': train_class_metrics,\n",
    "        'test': test_class_metrics\n",
    "    })\n",
    "    \n",
    "    # Print current fold metrics\n",
    "    print(\"\\nTrain Metrics:\")\n",
    "    for metric_name, value in train_metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nTest Metrics:\")\n",
    "    for metric_name, value in test_metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrices for this fold\n",
    "    plot_confusion_matrices(y_test, y_pred_test, race_columns)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot learning curves\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.axvline(x=np.argmin(history.history['val_loss']), color='r', linestyle='--', \n",
    "                label='Best Model')\n",
    "    plt.title('Model Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.axvline(x=np.argmin(history.history['val_loss']), color='r', linestyle='--', \n",
    "                label='Best Model')\n",
    "    plt.title('Model Accuracy Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Calculate and print final average results\n",
    "print(\"\\nFinal Average Results:\")\n",
    "print(\"\\nOverall Metrics:\")\n",
    "for metric_name in all_metrics[0]['test'].keys():\n",
    "    test_values = [fold['test'][metric_name] for fold in all_metrics]\n",
    "    mean_value = np.mean(test_values)\n",
    "    std_value = np.std(test_values)\n",
    "    print(f\"{metric_name}: {mean_value:.4f} ± {std_value:.4f}\")\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "for class_idx, class_name in enumerate(race_columns):\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    for metric_name in ['precision_per_class', 'recall_per_class', 'f1_per_class', 'roc_auc_per_class']:\n",
    "        test_values = [fold['test'][metric_name][class_idx] for fold in all_class_metrics]\n",
    "        mean_value = np.mean(test_values)\n",
    "        std_value = np.std(test_values)\n",
    "        print(f\"{metric_name}: {mean_value:.4f} ± {std_value:.4f}\")\n",
    "\n",
    "# Plot learning curves from the last fold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24bb0f8ed1a5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "\n",
    "# First, let's prepare our data\n",
    "# Get feature columns (excluding the Race_ columns)\n",
    "race_columns = [col for col in train_df.columns if col.startswith('Race_')]\n",
    "feature_columns = [col for col in train_df.columns if not col.startswith('Race_')]\n",
    "\n",
    "# Handle missing values in Age column\n",
    "train_df['Age'].fillna(train_df['Age'].mean(), inplace=True)\n",
    "\n",
    "# Prepare X and y\n",
    "X = train_df[feature_columns].values\n",
    "y = train_df[race_columns].values\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def create_model(input_dim, l2_lambda=0.0001):\n",
    "    initial_learning_rate = 0.001\n",
    "    decay_steps = 1000\n",
    "    decay_rate = 0.9\n",
    "    learning_rate_schedule = ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        staircase=True\n",
    "    )\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim,\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda)),\n",
    "        Dropout(0.05),\n",
    "        Dense(128, activation='relu',\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda)),\n",
    "        Dropout(0.05),\n",
    "        Dense(128, activation='relu',\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda)),\n",
    "        Dropout(0.05),\n",
    "        Dense(128, activation='relu',\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda)),\n",
    "        Dropout(0.05),\n",
    "        Dense(len(race_columns), activation='softmax',\n",
    "              kernel_regularizer=l2(l2_lambda),\n",
    "              bias_regularizer=l2(l2_lambda))\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate_schedule),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# [Rest of the code remains exactly the same]\n",
    "def plot_confusion_matrices(y_true, y_pred, class_names):\n",
    "    \"\"\"Plot confusion matrices for each class\"\"\"\n",
    "    fig, axes = plt.subplots(2, (len(class_names) + 1) // 2, figsize=(15, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        cm = confusion_matrix(y_true[:, idx], (y_pred[:, idx] > 0.5).astype(int))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=axes[idx])\n",
    "        axes[idx].set_title(f'Confusion Matrix - {class_name}')\n",
    "        axes[idx].set_xlabel('Predicted')\n",
    "        axes[idx].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def calculate_metrics(y_true, y_pred_proba):\n",
    "    \"\"\"Calculate comprehensive metrics for multi-label classification\"\"\"\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro'),\n",
    "        'precision_micro': precision_score(y_true, y_pred, average='micro'),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro'),\n",
    "        'recall_micro': recall_score(y_true, y_pred, average='micro'),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
    "        'f1_micro': f1_score(y_true, y_pred, average='micro'),\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_proba, average='macro'),\n",
    "        'accuracy': (y_true == y_pred).all(axis=1).mean()\n",
    "    }\n",
    "    \n",
    "    # Per-class metrics\n",
    "    class_metrics = {\n",
    "        'precision_per_class': precision_score(y_true, y_pred, average=None),\n",
    "        'recall_per_class': recall_score(y_true, y_pred, average=None),\n",
    "        'f1_per_class': f1_score(y_true, y_pred, average=None),\n",
    "        'roc_auc_per_class': roc_auc_score(y_true, y_pred_proba, average=None)\n",
    "    }\n",
    "    \n",
    "    return metrics, class_metrics\n",
    "\n",
    "# Perform 10-fold cross validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "all_metrics = []\n",
    "all_class_metrics = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\nFold {fold + 1}/10\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Scale features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_model(X_train.shape[1])\n",
    "    \n",
    "    # Define callbacks    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=f'model_checkpoints/model_fold_{fold+1}.keras',  # Changed from .h5 to .keras\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,     # Only save when the monitored metric is improved\n",
    "        mode='max',              # We want to minimize the validation loss\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model with callbacks\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=100,              # Maximum number of epochs\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        callbacks=[model_checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Load the best model for this fold\n",
    "    best_model = load_model(f'model_checkpoints/model_fold_{fold+1}.keras')\n",
    "    \n",
    "    # Get predictions using the best model\n",
    "    y_pred_train = best_model.predict(X_train_scaled)\n",
    "    y_pred_test = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_metrics, train_class_metrics = calculate_metrics(y_train, y_pred_train)\n",
    "    test_metrics, test_class_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    \n",
    "    # Store metrics\n",
    "    all_metrics.append({\n",
    "        'fold': fold + 1,\n",
    "        'train': train_metrics,\n",
    "        'test': test_metrics\n",
    "    })\n",
    "    \n",
    "    all_class_metrics.append({\n",
    "        'fold': fold + 1,\n",
    "        'train': train_class_metrics,\n",
    "        'test': test_class_metrics\n",
    "    })\n",
    "    \n",
    "    # Print current fold metrics\n",
    "    print(\"\\nTrain Metrics:\")\n",
    "    for metric_name, value in train_metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nTest Metrics:\")\n",
    "    for metric_name, value in test_metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrices for this fold\n",
    "    plot_confusion_matrices(y_test, y_pred_test, race_columns)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot learning curves\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.axvline(x=np.argmin(history.history['val_loss']), color='r', linestyle='--', \n",
    "                label='Best Model')\n",
    "    plt.title('Model Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.axvline(x=np.argmin(history.history['val_loss']), color='r', linestyle='--', \n",
    "                label='Best Model')\n",
    "    plt.title('Model Accuracy Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Calculate and print final average results\n",
    "print(\"\\nFinal Average Results:\")\n",
    "print(\"\\nOverall Metrics:\")\n",
    "for metric_name in all_metrics[0]['test'].keys():\n",
    "    test_values = [fold['test'][metric_name] for fold in all_metrics]\n",
    "    mean_value = np.mean(test_values)\n",
    "    std_value = np.std(test_values)\n",
    "    print(f\"{metric_name}: {mean_value:.4f} ± {std_value:.4f}\")\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "for class_idx, class_name in enumerate(race_columns):\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    for metric_name in ['precision_per_class', 'recall_per_class', 'f1_per_class', 'roc_auc_per_class']:\n",
    "        test_values = [fold['test'][metric_name][class_idx] for fold in all_class_metrics]\n",
    "        mean_value = np.mean(test_values)\n",
    "        std_value = np.std(test_values)\n",
    "        print(f\"{metric_name}: {mean_value:.4f} ± {std_value:.4f}\")\n",
    "\n",
    "# Plot learning curves from the last fold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
